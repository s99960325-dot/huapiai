import asyncio
import base64
Any, Dict, Literal, cast

import aiohttp
import requests
from pydantic import BaseModel, ConfigDict

import huapir.llm.format.tool as tool
from huapir.config.global_config import ModelConfig
from huapir.llm.adapter import AutoDetectModelsProtocol, LLMBackendAdapter, LLMChatProtocol, LLMEmbeddingProtocol
from huapir.llm.format.message import (LLMChatContentPartType, LLMChatImageContent, LLMChatMessage,
                                          LLMChatTextContent, LLMToolCallContent, LLMToolResultContent, RoleType)
from huapir.llm.format.request import LLMChatRequest, Tool
from huapir.llm.format.response import LLMChatResponse, Message, Usage
from huapir.llm.format.embedding import LLMEmbeddingRequest, LLMEmbeddingResponse
from huapir.llm.model_types import LLMAbility, ModelType
from huapir.logger import get_logger
from huapir.media import MediaManager
from huapir.tracing import trace_llm_chat

from .utils import generate_tool_call_id, pick_tool_calls

SAFETY_SETTINGS = [{
    "category": "HARM_CATEGORY_HARASSMENT",
    "threshold": "BLOCK_NONE"
}, {
    "category": "HARM_CATEGORY_HATE_SPEECH",
    "threshold": "BLOCK_NONE"
}, {
    "category": "HARM_CATEGORY_SEXUALLY_EXPLICIT",
    "threshold": "BLOCK_NONE"
}, {
    "category": "HARM_CATEGORY_DANGEROUS_CONTENT",
    "threshold": "BLOCK_NONE"
}, {
    "category": "HARM_CATEGORY_CIVIC_INTEGRITY",
    "threshold": "BLOCK_NONE"
}]

# POST 模式支持最大 20 MB 的 inline data
INLINE_LIMIT_SIZE = 1024 * 1024 * 20

IMAGE_MODAL_MODELS = [
    "gemini-2.0-flash-exp"
]


class GeminiConfig(BaseModel):
    api_key: str
    api_base: str = "https://generativelanguage.googleapis.com/v1beta"
    model_config = ConfigDict(frozen=True)


async def convert_non_tool_message(msg: LLMChatMessage, media_manager: MediaManager) -> dict:
    parts: list[dict[str, Any]] = []
    elements = cast(list[LLMChatContentPartType], msg.content)
    for element in elements:
        if isinstance(element, LLMChatTextContent):
            parts.append({"text": element.text})
        elif isinstance(element, LLMChatImageContent):
            media = media_manager.get_media(element.media_id)
            if media is None:
                raise ValueError(f"Media {element.media_id} not found")
            parts.append({
                "inline_data": {
                    "mime_type": str(media.mime_type),
                    "data": await media.get_base64()
                }
            })
        elif isinstance(element, LLMToolCallContent):
            parts.append({
                "functionCall": {
                    "name": element.name,
                    "args": element.parameters 
                }
            })
    return {
        "role": "model" if msg.role == "assistant" else "user",
        "parts": parts
    }

async def convert_llm_chat_message_to_gemini_message(msg: LLMChatMessage, media_manager: MediaManager) -> dict:
    if msg.role in ["user", "assistant", "system"]:
        return await convert_non_tool_message(msg, media_manager)
    elif msg.role == "tool":
        results = cast(list[LLMToolResultContent], msg.content)
        return {"role": "user", "parts": [resolve_tool_results(result) for result in results]}
    else:
        raise ValueError(f"Invalid role: {msg.role}")

async def convert_all_messages_to_gemini_format(messages: list[LLMChatMessage], media_manager: MediaManager) -> list[dict]:
    # gather需要先用异步函数封装，然后才能使用asyncio.run()
    return await asyncio.gather(*[convert_llm_chat_message_to_gemini_message(msg, media_manager) for msg in messages])

def convert_tools_to_gemini_format(tools: list[Tool]) -> list[dict[Literal["function_declarations"], list[dict]]]:
    # 定义允许的字段结构
    allowed_keys = {
        "name": True,
        "description": True,
        "parameters": {
            "type": True,
            "properties": {
                "*": {
                    "type": True,
                    "title": True,
                    "description": True,
                    "enum": True,
                    "default": True,
                    "items": True,
                }
            },
            "required": True
        }
    }

    def filter_dict(data: dict, allowed: dict) -> dict:
        """递归过滤字典，只保留允许的字段"""
        result = {}
        for key, value in allowed.items():
            if key == "*" and isinstance(value, dict):
                # 处理通配符情况，适用于 properties 字典
                for data_key, data_value in data.items():
                    if isinstance(data_value, dict):
                        result[data_key] = filter_dict(data_value, value)
                    else:
                        result[data_key] = data_value
            elif key in data:
                if isinstance(value, dict) and isinstance(data[key], dict):
                    # 如果是嵌套字典，递归处理
                    result[key] = filter_dict(data[key], value)
                else:
                    # 否则直接保留值
                    result[key] = data[key]
        return result

    function_declarations = []
    for tool in tools:
        # 将Tool对象转换为字典
        tool_dict = tool.model_dump()
        # 过滤出需要的字段
        filtered_tool = filter_dict(tool_dict, allowed_keys)
        function_declarations.append(filtered_tool)

    return [{"function_declarations": function_declarations}]

def resolve_tool_results(element: LLMToolResultContent) -> dict:
    # 全部拼接成字符串
    output = ""
    for content in element.content:
        if isinstance(content, tool.TextContent):
            output += content.text
        elif isinstance(content, tool.MediaContent):
            # FIXME: Gemini 不支持 response 传媒体内容，需要从额外的 message 中传入，类似于 **篡改记忆**
            output += f"<media id={content.media_id} mime_type={content.mime_type} />"
    return {
        "functionResponse": {
            "name": element.name,
            "response": {"error": output} if element.isError else {"output": output}
        }
    }

class GeminiAdapter(LLMBackendAdapter, AutoDetectModelsProtocol, LLMChatProtocol, LLMEmbeddingProtocol):

    media_manager: MediaManager

    def __init__(self, config: GeminiConfig):
        self.config = config
        self.logger = get_logger("GeminiAdapter")

    @trace_llm_chat
    def chat(self, req: LLMChatRequest) -> LLMChatResponse:
        api_url = f"{self.config.api_base}/models/{req.model}:generateContent?key={self.config.api_key}"
        headers = {
            # 这里的 api key 验证方法和 api reference 不一致。本次处理暂时按照api reference写法更正。 Warning: 未进行实际测试
            # "x-goog-api-key": self.config.api_key,
            "Content-Type": "application/json",
        }

        response_modalities = ["text"]
        if req.model in IMAGE_MODAL_MODELS:
            response_modalities.append("image")

        data = {
            "contents": asyncio.run(convert_all_messages_to_gemini_format(req.messages, self.media_manager)),
            "generationConfig": {
                "temperature": req.temperature,
                "topP": req.top_p,
                "topK": 40,
                "maxOutputTokens": req.max_tokens,
                "stopSequences": req.stop,
                "responseModalities": response_modalities,
            },
            "safetySettings": SAFETY_SETTINGS,
            "tools": convert_tools_to_gemini_format(req.tools) if req.tools else None,
        }
        
        self.logger.debug(f"Gemini request: {data}")

        # Remove None fields
        data = {k: v for k, v in data.items() if v is not None}

        response = self._post_with_retry(api_url, json=data, headers=headers)

        try:
            response_data = response.json()
        except Exception as e:
            self.logger.error(f"API Response: {response.text}")
            raise e
        content: list[LLMChatContentPartType] = []
        role = "assistant"
        for part in response_data["candidates"][0]["content"]["parts"]:
            if "text" in part:
                content.append(LLMChatTextContent(text=part["text"]))
            elif "inlineData" in part:
                decoded_image_data = base64.b64decode(part["inlineData"]["data"])
                media = asyncio.run(
                    self.media_manager.register_from_data(
                        data=decoded_image_data,
                        format=part["inlineData"]["mimeType"].removeprefix(
                            "image/"),
                        source="gemini response")
                )
                content.append(LLMChatImageContent(media_id=media))
            elif "functionCall" in part:
                content.append(
                    LLMToolCallContent(
                            id=generate_tool_call_id(part["functionCall"]["name"]), 
                            name=part["functionCall"]["name"], 
                            parameters=part["functionCall"].get("args", None)
                        )
                    )
    
        return LLMChatResponse(
            model=req.model,
            usage=Usage(
                prompt_tokens=response_data["usageMetadata"].get(
                    "promptTokenCount"),
                cached_tokens=response_data["usageMetadata"].get(
                    "cachedContentTokenCount"),
                completion_tokens=sum([modality.get(
                    "tokenCount", 0) for modality in response_data.get("promptTokensDetails", [])]),
                total_tokens=response_data["usageMetadata"].get(
                    "totalTokenCount"),
            ),
            message=Message(
                content=content,
                role=cast(RoleType, role),
                finish_reason=response_data["candidates"][0].get("finishReason"),
                tool_calls=pick_tool_calls(content)
            ),
        )
    
    def embed(self, req: LLMEmbeddingRequest) -> LLMEmbeddingResponse:
        # 使用批量嵌入接口，单次嵌入接口:embedContent
        # gemini 的 API reference 是这样定义的很奇怪，居然敢在 url 中传递key
        api_url = f"{self.config.api_base}/models/{req.model}:batchEmbedContents?key={self.config.api_key}"
        headers = {
            "Content-Type": "application/json",
        }
        # 目前 gemini 没有一个嵌入模型支持多模态嵌入
        if  any(isinstance(input, LLMChatImageContent) for input in req.inputs):
            raise ValueError("gemini does not support multi-modal embedding")
        inputs = cast(list[LLMChatTextContent], req.inputs)
        data = [ 
            {
                "model": req.model,
                "content": {
                    "parts": [{"text": input.text}]
                },
                "outputDimensionality": req.dimension
            } for input in inputs
        ]
        # 移除None字段
        data = [{ k:v for k,v in item.items() if v is not None} for item in data]
        response = self._post_with_retry(url=api_url,json={"requests": data}, headers=headers)
        try:
            # {
            #     "embeddings": [
            #         {"values": [0.1, ...]},
            #         ...
            #     ]
            # }
            response_data: dict[Literal["embeddings"],list[dict[Literal["values"], list[float]]]] = response.json()
        except Exception as e:
            self.logger.error(f"API Response: {response.text}")
            raise e
        return LLMEmbeddingResponse(
            # gemini不返回usage
            vectors=[data["values"] for data in response_data["embeddings"]]
        )

    async def auto_detect_models(self) -> list[ModelConfig]:
        api_url = f"{self.config.api_base}/models"
        async with aiohttp.ClientSession(trust_env=True) as session:
            async with session.get(
                api_url, headers={"x-goog-api-key": self.config.api_key}
            ) as response:
                if response.status != 200:
                    self.logger.error(f"获取模型列表失败: {await response.text()}")
                    response.raise_for_status()
                response_data = await response.json()
                return [
                    ModelConfig(id=model["name"].removeprefix("models/"), type=ModelType.LLM.value, ability=LLMAbility.TextChat.value)
                    for model in response_data["models"]
                    if "generateContent" in model["supportedGenerationMethods"]
                ]

    def _post_with_retry(self, url: str, json: dict, headers: dict, retry_count: int = 3) -> requests.Response: # type: ignore
        for i in range(retry_count):
            try:
                response = requests.post(url, json=json, headers=headers)
                response.raise_for_status()
                return response
            except requests.exceptions.RequestException as e:
                if i == retry_count - 1:
                    self.logger.error(
                        f"API Response: {response.text if 'response' in locals() else 'No response'}")
                    raise e
                else:
                    self.logger.warning(
                        f"Request failed, retrying {i+1}/{retry_count}: {e}")
