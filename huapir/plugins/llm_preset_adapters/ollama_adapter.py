import asyncio
Any, cast, Dict

import aiohttp
import requests
from pydantic import BaseModel, ConfigDict

import huapir.llm.format.tool as tools
from huapir.config.global_config import ModelConfig
from huapir.llm.adapter import AutoDetectModelsProtocol, LLMBackendAdapter, LLMChatProtocol, LLMEmbeddingProtocol
from huapir.llm.format.message import (LLMChatContentPartType, LLMChatImageContent, LLMChatMessage,
                                          LLMChatTextContent, LLMToolCallContent, LLMToolResultContent)
from huapir.llm.format.request import LLMChatRequest, Tool
from huapir.llm.format.response import LLMChatResponse, Message, Usage
from huapir.llm.format.embedding import LLMEmbeddingRequest, LLMEmbeddingResponse
from huapir.llm.model_types import LLMAbility, ModelType
from huapir.logger import get_logger
from huapir.media.manager import MediaManager
from huapir.tracing import trace_llm_chat

from .openai_adapter import convert_tools_to_openai_format
from .utils import generate_tool_call_id, pick_tool_calls


class OllamaConfig(BaseModel):
    api_base: str = "http://localhost:11434"
    model_config = ConfigDict(frozen=True)


async def resolve_media_ids(media_ids: list[str], media_manager: MediaManager) -> list[str]:
    result = []
    for media_id in media_ids:
        media = media_manager.get_media(media_id)
        if media is not None:
            base64_data = await media.get_base64()
            result.append(base64_data)
    return result

def convert_llm_response(response_data: dict[str, dict[str, Any]]) -> list[LLMChatContentPartType]:
    # 通过实践证明 llm 调用工具时 content 字段为空字符串没有任何有效信息不进行记录
    if calls := response_data["message"].get("tool_calls", None):
        return [LLMToolCallContent(
            id=generate_tool_call_id(call["function"]["name"]),
            name=call["function"]["name"],
            parameters=call["function"].get("arguments", None)
        ) for call in calls
        ]
    else:
        return [LLMChatTextContent(text=response_data["message"].get("content", ""))]

def convert_non_tool_message(msg: LLMChatMessage, media_manager: MediaManager, loop: asyncio.AbstractEventLoop) -> dict[str, Any]:
    text_content = ""
    images: list[str] = []
    tool_calls: list[dict[str, Any]] = []
    messages: dict[str, Any] = {
        "role": msg.role,
        "content": "",
    }
    for part in msg.content:
        if isinstance(part, LLMChatTextContent):
            text_content += part.text
        elif isinstance(part, LLMChatImageContent):
            images.append(part.media_id)
        elif isinstance(part, LLMToolCallContent):
            tool_calls.append({
                "function": {
                    "name": part.name,
                    "arguments": part.parameters,
                }
            })
    messages["content"] = text_content
    if images:
        messages["images"] = loop.run_until_complete(
            resolve_media_ids(images, media_manager))
    if tool_calls:
        messages["tool_calls"] = tool_calls
    return messages


def convert_tool_result_message(msg: LLMChatMessage, media_manager: MediaManager, loop: asyncio.AbstractEventLoop) -> list[dict]:
    """
    将工具调用结果转换为 Ollama 格式
    """
    elements = cast(list[LLMToolResultContent], msg.content)
    messages = []
    for element in elements:
        output = ""
        for item in element.content:
            if isinstance(item, tools.TextContent):
                output += f"{item.text}\n"
            elif isinstance(item, tools.MediaContent):
                output += f"<media id={item.media_id} mime_type={item.mime_type} />\n"
        if element.isError:
            output = f"Error: {element.name}\n{output}"
        messages.append({"role": "tool", "content": output,
                        "tool_call_id": element.id})
    return messages

def convert_tools_to_ollama_format(tools: list[Tool]) -> list[dict]:
    # 这里将其独立出来方便应对后续接口改动
    return convert_tools_to_openai_format(tools)

class OllamaAdapter(LLMBackendAdapter, AutoDetectModelsProtocol, LLMChatProtocol, LLMEmbeddingProtocol):
    def __init__(self, config: OllamaConfig):
        self.config = config
        self.logger = get_logger("OllamaAdapter")

    @trace_llm_chat
    def chat(self, req: LLMChatRequest) -> LLMChatResponse:
        api_url = f"{self.config.api_base}/api/chat"
        headers = {"Content-Type": "application/json"}

        # 将消息转换为 Ollama 格式
        messages = []
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        for msg in req.messages:
            # 收集每条消息中的文本内容和图像
            if msg.role == "tool":
                messages.extend(convert_tool_result_message(
                    msg, self.media_manager, loop))
            else:
                messages.append(convert_non_tool_message(
                    msg, self.media_manager, loop))

        data = {
            "model": req.model,
            "messages": messages,
            "stream": False,
            "options": {
                "temperature": req.temperature,
                "top_p": req.top_p,
                "num_predict": req.max_tokens,
                "stop": req.stop,
                "tools": convert_tools_to_ollama_format(req.tools) if req.tools else None,
            },
        }

        # Remove None fields
        data = {k: v for k, v in data.items() if v is not None}
        if "options" in data:
            data["options"] = {
                k: v for k, v in data["options"].items() if v is not None # type: ignore
            }

        response = requests.post(api_url, json=data, headers=headers)
        try:
            response.raise_for_status()
            response_data = response.json()
        except Exception as e:
            self.logger.error(f"API Response: {response.text}")
            raise e
        # https://github.com/ollama/ollama/blob/main/docs/api.md#generate-a-chat-completion
        content = convert_llm_response(response_data)
        return LLMChatResponse(
            model=req.model,
            message=Message(
                content=content,
                role="assistant",
                finish_reason="stop",
                tool_calls=pick_tool_calls(content),
            ),
            usage=Usage(
                prompt_tokens=response_data['prompt_eval_count'],
                completion_tokens=response_data['eval_count'],
                total_tokens=response_data['prompt_eval_count'] +
                response_data['eval_count'],
            )
        )

    def embed(self, req: LLMEmbeddingRequest) -> LLMEmbeddingResponse:
        # https://github.com/ollama/ollama/blob/main/docs/api.md#generate-embeddings api文档地址
        api_url = f"{self.config.api_base}/api/embed"
        headers = {"Content-Type": "application/json"}
        if any(isinstance(input, LLMChatImageContent) for input in req.inputs):
            raise ValueError("ollama api does not support multi-modal embedding")
        inputs = cast(list[LLMChatTextContent], req.inputs)
        data = {
            "model": req.model,
            "input": [input.text for input in inputs],
            # 禁止自动截断输入数据用以适应上下文长度
            "truncate": req.truncate
        }
        data = { k:v for k, v in data.items() if v is not None }
        response = requests.post(api_url, json=data, headers=headers)
        try:
            response.raise_for_status()
            response_data = response.json()
        except Exception as e:
            self.logger.error(f"API Response: {response.text}")
            raise e
        return LLMEmbeddingResponse(
            vectors=response_data["embeddings"],
            usage=Usage(
                prompt_tokens=response_data.get("prompt_eval_count", 0)
            )
        )

    async def auto_detect_models(self) -> list[ModelConfig]:
        api_url = f"{self.config.api_base}/api/tags"
        async with aiohttp.ClientSession(trust_env=True) as session:
            async with session.get(api_url) as response:
                response.raise_for_status()
                response_data = await response.json()
                return [ModelConfig(id=tag["name"], type=ModelType.LLM.value, ability=LLMAbility.TextChat.value) for tag in response_data["models"]]
